{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "\n",
    "We start with the fundamental assumption of machine learning:\n",
    "\n",
    "> The data that you train your model on must come from the same distribution as the data you hope to apply the model to.\n",
    "\n",
    "And there's is no way around this. One million observations of the wrong thing is not going to help you build a model for the thing you want. This is just the Machine Learning version of the idea that a sample must be representative. Assuming you have the right thing, though, how do you calculate a metric for it that gives you an idea of how it will work on unseen data?\n",
    "\n",
    "To a certain extent, we have already been wrestling with this problem.\n",
    "We have a *sample* of data from a process but we don't know for sure all the values the process can or will ever generate (\"population\").\n",
    "And while we need to assume that, in general, the process is stable for us to even think about modeling it, the problem of inference remains.\n",
    "We have so far looked at the problem of inference as, \"given that the data is potentially consistent with a large number of models, which model(s) should I find the most credible?\"\n",
    "This is the Bayesian perspective.\n",
    "\n",
    "The Machine Learning perspective is, \"how will the model most likely perform on previously unseen data?\".\n",
    "What do we mean by unseen data?\n",
    "Do we mean that the value $7.3$ never appeared in the training data? No.\n",
    "We generally mean, the model is not evaluated on the same data that was used to train it.\n",
    "As we noted previously, the data still has to come from the same distribution.\n",
    "Where can we get this unseen data?\n",
    "We've already noted that getting more data (additional samples) can be difficult, costly, or impossible.\n",
    "\n",
    "One very simple way would be to split the data we have into a train(ing) set and a test set.\n",
    "We could train our model (say, linear regression) on the train set and evaluated it (calculate MSE) on the test set.\n",
    "We then get an estimate of MSE on unseen data for our model.\n",
    "\n",
    "Unfortunately, that only gets use a single estimate and we'd like to get a sense of the overall variation in performance.\n",
    "We could switch the roles of train and test sets, train on the test set and evaluated on the train set and get another estimate of MSE but two isn't quite enough either.\n",
    "Enter *cross validation*.\n",
    "\n",
    "Borrowing ideas from bootstrapping, there's another option. You could divide your data into ten sections or *folds* and then loop over each fold, using it as the test set and the others as the training set. So, for the first iteration, you have Fold 1 as the test set and Folds 2-10 (combined) as the training set. The second iteration sees Fold 2 as the test set and Folds 1, 3-10 (combined) as the training set. And so on.\n",
    "\n",
    "What this looks like is as follows:\n",
    "\n",
    "|  1  |  2  |  3  |  4  |  5  |  6  |  7  |  8  |  9  |  10 |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| **Test** | Train | Train | Train | Train | Train | Train | Train | Train | Train |\n",
    "\n",
    "|  1  |  2  |  3  |  4  |  5  |  6  |  7  |  8  |  9  |  10 |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| Train| **Test** | Train | Train | Train | Train | Train | Train | Train | Train |\n",
    "\n",
    "|  1  |  2  |  3  |  4  |  5  |  6  |  7  |  8  |  9  |  10 |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| Train| Train | **Test** | Train | Train | Train | Train | Train | Train | Train |\n",
    "\n",
    "And finally:\n",
    "\n",
    "|  1  |  2  |  3  |  4  |  5  |  6  |  7  |  8  |  9  |  10 |\n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n",
    "| Train| Train | Train | Train | Train | Train | Train | Train | Train | **Test** |\n",
    "\n",
    "When you're done, you would then end up with 10 estimates of MSE. With ten estimates of MSE we're back into familiar statistical inference territory, we can estimate the mean MSE (which sounds funny if you say \"mean Mean Squared Error\") and the variance.\n",
    "\n",
    "There are other variants of cross-validation such as 3 fold and 5 fold. The *main* consideration on the number of folds is this: is each fold representative of the distribution you're trying to model? This often depends on:\n",
    "\n",
    "1. How many observations do you have? 300 or 300 million?\n",
    "2. How many variables do you have? The more variables you have, the \"smaller\" your data is from a machine learning perspective.\n",
    "3. How many values can each categorical variable take? Each possible value of a categorical variable is a partitioning of your data. If you have a lot of categorical variables which can each take on many values, your 300 million observations may actually *not* be enough data.\n",
    "\n",
    "There has been quite a bit of research on cross validation in general and how well it works. From my own vantage point of seeing students apply 10 fold cross validation to the same problem, I would suggest doing successive rounds of cross validation, creating new folds each time. You should do as many rounds as needed to reach at least 30 estimates of your metric.\n",
    "\n",
    "For 10 fold cross validation, that's three (3) rounds of 10 fold-cross validation. This is nice because because with 30 observations, you can do Bayesian bootstrap inference on your results, and estimate the posterior distribution of the mean of your evaluation metric.\n",
    "\n",
    "Why not just do bootstrap sampling directly by taking a random 10% sample as the test set, 100s of times?\n",
    "I often do this because it's easier than setting up folds.\n",
    "However, the literature gives this approach mixed reviews.\n",
    "\n",
    "Which leads us to one final world of advice: *randomize*. Never use your data just as you found it. Make sure you randomize your data each time before establishing your folds (in other words, *shuffle* your data each time before you designate folds)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "It's not difficult to implement N-Fold Cross Validation on your own.\n",
    "You can select the indices of your dataframe (or data), randomize them, \"chunk\" them into N batches, and iterate.\n",
    "However, Scikit Learn does implement [N-Fold Cross Validation](https://scikit-learn.org/stable/modules/cross_validation.html).\n",
    "You will have to work the Scikit Learn's classes directly instead of the `models` module."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (en685648)",
   "language": "python",
   "name": "en685648"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "30px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
