{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Model evaluation takes on an entirely new dimension for the Data Scientist. At some point, someone will say to you, \"we want to increase engagement\" and that's fine, except that we need to agree on what that means in a very concrete way. Does it mean time on the page or number of pages visited? You need to see if that data is being collected and if it can be calculated. This should have been part of the original CoNVO or subsequent improvements to the understanding of the problem. It may have been missed.\n",
    "\n",
    "Let us take an example from the very first module, you have a news site and you want people to read the articles. One way you think you can do that is to build a model that scores each story for each individual, producing a propensity to read the story. This is a complex classification/information retrieval problem. So when building the model, you might conduct some cross validation studies to see how well the model performs.\n",
    "\n",
    "But \"reading propensity\" may not be the metric you're trying to optimize. So in the A/B Test, the performance metric diverges from the loss function and performance metric of the model. Your organization may have a whole slate of engagement scores. Your model was not built to optimize all of those and in a dynamic context.\n",
    "\n",
    "Put differently, there's a difference between the estimation of response variables (and associated loss functions) and evaluation metrics. The response variable of your model may be the probability that someone buys a certain product. The evaluation metric of the model may be purchase rates. These are not the same thing.\n",
    "\n",
    "A/B Testing may ultimately be used to test what is more important and only validates the model in a very indirect sort of way.\n",
    "\n",
    "## Summary\n",
    "\n",
    "Here are a few take away points:\n",
    "\n",
    "1. High Bias/High Variance are not absolutes. There is no algorithm that produces models that are always high bias. High bias or variance exist in context with the inputs, metaparameters, and data.\n",
    "2. There is a lot of trial and error in \"debugging\" a machine learning model. It's a very large search space of possibilities. Use EDA to inform your search. We knew to try a quadratic transformation on $x_1$ because we'd looked at the data. Note that some companies, DataRobot for example, are trying to automate the process by automatically searching the grid of possibilities using huge clusters in the cloud. You  might be able to accomplish something similar on your own.\n",
    "3. Real life curves--learning or validation--rarely look as neat as the textbook versions.\n",
    "\n",
    "## Review\n",
    "\n",
    "\n",
    "1. Describe the two cultures of model evaluation. How are they different?\n",
    "2. What is the most typical evaluation metric for regression (value prediction) problems? What is the formula?\n",
    "3. What is a confusion matrix? What are each of the elements in the matrix?\n",
    "4. Define and provide the formula for:\n",
    "  1. accuracy\n",
    "  2. error rate \n",
    "  3. sensitivity/true positive rate\n",
    "  4. specificity\n",
    "  5. precision\n",
    "5. How does 10 fold cross validation work? How would you apply it to linear or logistic regression?\n",
    "6. What is the bias/variance tradeoff?\n",
    "7. How does bias relate to underfitting?\n",
    "8. How does variance relate to overfitting?\n",
    "9. What are the five general ways in which a \"model\" can be improved?\n",
    "10. Describe how to calculate learning curves. What do they tell you about your model?\n",
    "11. What do learning curves look like when your current situation involves high bias?\n",
    "12. What do learning curves look like when your current situation involves high variance?\n",
    "13. If your learning curves indicate high bias, what does that suggest about getting more data? What should you do?\n",
    "14. If your learning curves indicate high variance, what does that suggest about getting more data? What should you do?\n",
    "13. What are validation curves used for? How do you interpret them?\n",
    "14. What are some other ways you can improve your model under high bias/high variance?\n",
    "15. What is regularization?\n",
    "16. What is the difference between \"backtesting\" (cross validation) and A/B testing?\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (en685648)",
   "language": "python",
   "name": "en685648"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
