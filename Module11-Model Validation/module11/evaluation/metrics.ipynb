{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "import patsy\n",
    "\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('resources')\n",
    "import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss and Evaluation Metrics\n",
    "\n",
    "If we are interested solely in prediction, what does it mean for a model to be good or bad? We talked about this before in terms of the general linearized model but the question--and possible answers--is more general. \n",
    "\n",
    "For example, we could simply predict $y$ by using some constant value of $y$, $\\breve{y}$. What $\\breve{y}$ should we use? This depends on how much it costs us to be wrong. What \"wrong\" means depends on the value we're trying to predict or, more generally, if we have a regression problem (predicting a numeric value) or a classification problem (predicting a categorical value)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "We talked about loss and regression before. If we define loss (or error) as $y - \\hat{y}$ and we wish to summarize this loss over some training set, then we have the following \"usual suspects\":\n",
    "\n",
    "* **squared loss**\n",
    "\n",
    "$L(y, \\hat{y}) = (y - \\hat{y})^2$\n",
    "\n",
    "* **absolute loss**\n",
    "\n",
    "$L(y, \\hat{y}) = |y - \\hat{y}|$\n",
    "\n",
    "* **0/1 loss**\n",
    "\n",
    "$L(y, \\hat{y}) = 1.0$ if $y = \\hat{y}$ else $0.0$\n",
    "\n",
    "All of these can be turned into a normalized summary statistic by taking the sum and averaging:\n",
    "\n",
    "$\\frac{1}{n} \\sum L(y, \\hat{y})$\n",
    "\n",
    "But these aren't the only possibilities. You can use other metrics such as $\\sigma$ or $R^2$. Additionally, you can calculate your own Loss function. For example, imagine that you want to penalize over-estimation *more* than underestimation:\n",
    "\n",
    "$$L(y, \\hat{y}) =\n",
    "\\begin{cases}\n",
    "(y - \\hat{y})^2,  & \\hat{y} > y \\\\\n",
    "|y - \\hat{y}|, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This last approach can be a bit tricky. The algorithm to calculate the coefficients of the typical linear regression finds coefficients that minimize *mean squared error* or MSE. If you have your own metric you wish to minimize, you may need to implement your own algorithm using, for example, stochastic gradient techniques.\n",
    "\n",
    "Most of the time when talking about regression problems (and not just linear regression) we will be using mean squared error, $\\sigma$, or $R^2$ as our evaluation metrics. However, I don't want you to get the impression that these are the only possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Like regression, classification has a baseline for predicting the class (or class *label*). We talked about this Null model as the relative frequency of the most common class. Technically, this would be $p$ if $p$ > 50% or $1-p=q$ if $q$ > 50%. For example, suppose that for a binary problem, the relative frequency of the most common class is 87.3%. Then this is the Null model for classification. If you guess \"1\" for any observation you see then you are right 87.3% of the time and wrong 12.7% of the time. This is the Null model for binary classification.\n",
    "\n",
    "For a multi-class problem, we pick the $p_i$ with the highest value. Although we called this the Null model in the previous chapters, in machine learning this is known as *OneR*. We estimate OneR simply by calculating the relative frequencies of the class labels and picking the label with the highest relative frequency.\n",
    "\n",
    "It is possible to formulate a variety of loss functions for the classification task.  Taking the *binary* case, *cross entropy* is one such function and the one used to find the $\\beta$s in logistic regression.\n",
    "\n",
    "$L(y, \\hat{y}) = y log( \\hat{y}) + (1 - y) log( 1 - \\hat{y})$\n",
    "\n",
    "and there are [others](https://en.wikipedia.org/wiki/Loss_functions_for_classification). However, I never see these functions used to *evaluate* classification models. They are normally used by the *algorithms*, $g(X, y)$, to learn the classification models, $f(X)$, from data.\n",
    "\n",
    "Instead, there are a variety of classification *metrics* that are used to evaluate how good or bad a classification model is. The reason for this plethora of metrics is that there appear to be a number of ways that a classification prediction can go right or wrong and different ways to summarize these outcomes. For a binary classification task, where \"1\" is taken to mean \"positive\" or \"in the class\" and \"0\" is taken to be \"negative\" or \"not in the class\", the possible cases for a classification model are:\n",
    "\n",
    "1. The true class can be \"1\" and the model can predict \"1\". This is a *true positive* or TP.\n",
    "2. The true class can be \"1\" and the model can predict \"0\". This is a *false negative* or FN.\n",
    "3. The true class can be \"0\" and the model can predict \"1\". This is a *false positive* or FP.\n",
    "4. The true class can be \"0\" and the model can predict \"0\". This is a *true negative* or TN.\n",
    "\n",
    "We can summarize these results in a table called a *confusion matrix*...it summarizes how confused (or not) our model is:\n",
    "\n",
    "|  &nbsp;      | Predict 1 | Predict 0|\n",
    "|:------------:|:--------:|:---------:|\n",
    "| **Actual 1** | TP       | FN        |\n",
    "| **Actual 0** | FP       | TN        |\n",
    "\n",
    "Since N is simply the number of observations TP + FP + FN + TN, then we have the following:\n",
    "\n",
    "**accuracy** = $\\frac{TP + TN}{N}$ = 1 - error rate\n",
    "\n",
    "**error rate** = $\\frac{FP + FN}{N}$ = 1 - accuracy\n",
    "\n",
    "which are the two most common metrics for evaluating classification models. We used this in previous chapters to evaluate logistic regression.\n",
    "\n",
    "However, they may not be sufficient. As we see above, there are two types of errors: classifying something that's \"1\" as \"0\" and classifying something that's \"0\" as \"1\". The impact of these errors may not be symmetric or equally costly. We saw this when talking about Bayes Theorem. A test may be good at telling you there's a problem if there really is a problem but they also tell you when there's a problem when there really *isn't* a problem, i.e., false alarms.\n",
    "\n",
    "These various possibilities each have their own names (sometimes several) and can be discussed in terms of the confusion matrix. We're going to look at the three more common ones.\n",
    "\n",
    "The first is **sensitivity**. This is basically the \"disease\" case: if you have the disease, how good is the model (test) at detecting it? It is also called **true positive rate**, **hit rate**, and **recall**. It is defined only in terms of the positive observations, both those the model predicted correctly and those it did not.\n",
    "\n",
    "$sensitivity = \\frac{TP}{TP+FN}$ (1 - sensitivity is the **false negative rate**)\n",
    "\n",
    "The next one is **specificity**. This is the other case. If you do not have the disease, how good is the model (test) at determining that (and not telling you that you do have it!)?\n",
    "\n",
    "$specificity = \\frac{TN}{TN+FP}$ (1 - specificity is the **false positive rate**)\n",
    "\n",
    "The final case is **precision**  or **positive predictive value**. Basically, of the positive (\"1\") predictions that we made, how many were right?\n",
    "\n",
    "$precision = \\frac{TP}{TP + FP}$\n",
    "\n",
    "When we're not looking at accuracy/error rate, we generally look at sensitivity and precision. In fact, there's a harmonic mean of the two called $F1$:\n",
    "\n",
    "$F1 = \\frac{2TP}{2TP + FP + FN}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Let's look at our \"switching\" logistic regression and evaluate it in terms of these new metrics. Let's load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells = pd.read_csv( \"resources/arsenic.wells.tsv\", sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells[ \"dist10\"] = wells[ \"dist\"].apply( lambda x: int( round( x / 10, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One possible model might have been:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: switch ~ dist10 + arsenic + assoc + educ\n",
      "-------------  ---------  -----\n",
      "Coefficients              Value\n",
      "               $\\beta_0$  -0.16\n",
      "dist10         $\\beta_1$  -0.09\n",
      "arsenic        $\\beta_2$  0.47\n",
      "assoc          $\\beta_3$  -0.13\n",
      "educ           $\\beta_4$  0.04\n",
      "\n",
      "Metrics        Value\n",
      "Error ($\\%$)   38.41\n",
      "Efron's $R^2$  0.07\n",
      "-------------  ---------  -----\n"
     ]
    }
   ],
   "source": [
    "result1 = models.logistic_regression( \"switch ~ dist10 + arsenic + assoc + educ\", data = wells)\n",
    "print(models.simple_describe_lgr(result1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error rate here was calculated to be 38.41%. The `logistic regression` function has been enriched to return both the $y$ and $\\hat{y}$ values to us. Let's use them to generate a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_confusion_matrix(result):\n",
    "    tp = 0; tn = 0; fn = 0; fp = 0\n",
    "    for y, y_hat in zip(result[\"y\"], result[\"y_hat\"]):\n",
    "        if y == 0 and y_hat == 0:\n",
    "            tn += 1\n",
    "        elif y == 0 and y_hat == 1:\n",
    "            fp += 1\n",
    "        elif y == 1 and y_hat == 1:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fn += 1\n",
    "    return tabulate([[\"Predicted 1\", tp, fp], [\"Predicted 0\", fn, tn]], headers=[\"\", \"Actual 1\", \"Actual 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Actual 1    Actual 0\n",
      "-----------  ----------  ----------\n",
      "Predicted 1        1391         814\n",
      "Predicted 0         346         469\n"
     ]
    }
   ],
   "source": [
    "print(binary_confusion_matrix(result1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that our true positives were 1391. Our true negatives were 469. For our errors, our false positives were 814 and our false negatives were 346. \n",
    "\n",
    "* Of all the actual negatives, how many did we predict to be positive?\n",
    "\n",
    "This the *false positive rate*: FP/(TN + FP) = 814/(814+469) = 63.4%.\n",
    "\n",
    "* Of all the actual positives, how many did we predict to be negative?\n",
    "\n",
    "The false negatives were 346 and the *false negative rate* was FN/(TP + FN) = 346/(1391+346) = 19.9%. \n",
    "\n",
    "By comparing these two rates, we can see that our model errs more often by predicting someone will switch when they didn't rather than predicting that someone will stay when they switched.\n",
    "\n",
    "* Of all the positive predictions, how many were correct?\n",
    "\n",
    "Finally, we can look at precision: TP/(TP+FP) = 1391/(1391 + 814) = 63.1%. Of our positive predictions (predicting switch), we were 63.1% correct.\n",
    "\n",
    "As you can see, this gives a more nuanced view of the model than just *error rate*. The main problem here is the false positive rate. We are much more likely to predict a switch than is actually reflected in the data on people switching. For something like arsenic poisoning, this is probably more important than underestimating those who do switch (false negative rate). If this were a government program, you'd need a two pronged approach. A better model of understanding the behavior that leads to switching but monitoring of actual switches to know if the program is successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrices work even for multi-class problems. Although the language isn't quite the same (you can't have true \"positives\" with three or more classes), there is still the sense of:\n",
    "\n",
    "1. Of actual class $i$, how many did we predict to be something else?\n",
    "2. Of predicted class $i$, how many were correct?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (en685648)",
   "language": "python",
   "name": "en685648"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "84px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
