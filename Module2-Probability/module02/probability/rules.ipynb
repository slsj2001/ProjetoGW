{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rules of Probability\n",
    "\n",
    "There are some handy rules that follow from the axioms of probability. In a probability course, you might be required to prove them. We will present them without proof."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monotonicity\n",
    "\n",
    "$A \\subseteq B \\Rightarrow P(A) \\leq P(B)$\n",
    "\n",
    "This says that if A is a subset of B then the probablity of A must not exceed the probability of B. This is really an abuse of notation. What we really mean is:\n",
    "\n",
    "$A \\subseteq B \\Rightarrow \\sum_i P(a_i) \\leq \\sum_j P(b_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negation\n",
    "\n",
    "$P(\\neg{a}) = 1 - P(a)$\n",
    "\n",
    "This follows from axiom #2. If we write out the summation and isolate the single event $a$ and then re-arrange terms, we get the above rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Probability\n",
    "\n",
    "This is also called the Law of Total Probability. It has the form:\n",
    "\n",
    "$P(A) = \\sum_i P(A|B=b_i) P(B=b_i)$\n",
    "\n",
    "Remember that P(A) is a set of probabilities (one for each element in A). This rules makes a bit more sense if you break it out:\n",
    "\n",
    "$P(A=a_1) = P(A=a_1|B=b_1) P(B=b_1) + P(A=a_1|B=b_2) P(B=b_2) + \\ldots + P(A=a_1|B=b_n) P(B=b_n)$\n",
    "\n",
    "We can derive the Law by looking at the definition of conditional probability for a single event:\n",
    "\n",
    "$P(A=a_1|B=b_1) = \\frac{P(A=a_1, B=b_1)}{P(B=b_1)}$\n",
    "\n",
    "and re-arranging terms:\n",
    "\n",
    "$P(A=a_1, B=b_1) = P(A=a_1|B=b_1)P(B=b_1)$\n",
    "\n",
    "From our discussion about marginalization and Axiom #2, we know that:\n",
    "\n",
    "$P(A=a_1) = P(A=a_1, B=b_1) + P(A=a_1, B=b_2) + \\ldots + P(A=a_1, B=b_n)$\n",
    "\n",
    "By substitution, we have:\n",
    "\n",
    "$P(A=a_1) = P(A=a_1|B=b_1) P(B=b_1) + P(A=a_1|B=b_2) P(B=b_2) + \\ldots + P(A=a_1|B=b_n) P(B=b_n)$\n",
    "\n",
    "and for the entire set A, we have:\n",
    "\n",
    "$P(A) = SUM_i P(A|B=b_i) P(B=b_i)$\n",
    "\n",
    "Total Probability is very useful (believe it or not) because there are many situations where you don't know P(A) but you know P(A|B) and P(B). We will see this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain Rule\n",
    "\n",
    "Again, starting with the definition of conditional probability we have:\n",
    "\n",
    "$P(A|B) = \\frac{P(A, B)}{P(B)}$\n",
    "\n",
    "and by re-arranging we have:\n",
    "\n",
    "$P(A, B) = P(A|B)P(B)$\n",
    "\n",
    "We can expand this to more sets:\n",
    "\n",
    "$P(A,B,C) = P(A|B,C) P(B|C) P(C)$\n",
    "\n",
    "$P(A,B,C,D) = P(A|B,C,D) P(B|C,D) P(C|D) P(D)$\n",
    "\n",
    "We can expand in any order:\n",
    "\n",
    "$P(A,B,C,D) = P(D|A,B,C) P(B|A,C) P(A|C) P(C)$\n",
    "\n",
    "But this is really just a clever manipulation of the definition of conditional probability:\n",
    "\n",
    "$P(A,B,C,D) = P(D|A,B,C) P(B|A,C) P(A|C) P(C)$\n",
    "$P(A,B,C,D) = \\frac{P(D,A,B,C)}{P(B,A,C)} \\frac{P(B,A,C)}{P(A, C)}\\frac{P(A, C)}{P(C)} P(C)$\n",
    "\n",
    "Still, it can be a handy thing to know and it presents the foundation for Bayesian Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes Rule\n",
    "\n",
    "Bayes Rule is a similar manipulation of conditional probability. We start with the definition of conditional probability:\n",
    "\n",
    "$P(A|B) = \\frac{P(A,B)}{P(B)}$\n",
    "\n",
    "and re-arrange:\n",
    "\n",
    "$P(A,B) = P(A|B)P(B)$\n",
    "\n",
    "We can start with the \"opposite\" conditional probability:\n",
    "\n",
    "$P(B|A) = \\frac{P(A,B)}{P(A)}$\n",
    "\n",
    "and re-arrange:\n",
    "\n",
    "$P(A,B) = P(B|A)P(A)$\n",
    "\n",
    "which means I can set these two equal to each other:\n",
    "\n",
    "$P(B|A)P(A) = P(A|B)P(B)$\n",
    "\n",
    "and re-arrange:\n",
    "\n",
    "$P(B|A) = \\frac{P(A|B)P(B)}{P(A)}$\n",
    "\n",
    "which does not look particularly interesting until I start giving my sets interesting names: B=Hypothesis and A=Data:\n",
    "\n",
    "$P(H|D) = \\frac{P(D|H)P(H)}{P(D)}$\n",
    "\n",
    "which says...the probability of the hypothesis (or model or parameter) *given* the data is equal to the probability of the data *given* the hypothesis times the probability of the hypothesis. This is the normalized by the probability of the data.\n",
    "\n",
    "These probabilities all have names:\n",
    "\n",
    "* P(H|D) = posterior probability\n",
    "* P(D|H) = likelihood\n",
    "* P(H)   = prior probability\n",
    "* P(D)\t = normalizer\n",
    "\n",
    "Note that we almost never know P(D) and we will often use total probability to calculate it:\n",
    "\n",
    "$P(D) = \\sum_h P(D|H=h)P(H=h)$\n",
    "\n",
    "Remember what our notation means. By solving for $P(H|D)$ we are solving for all the hypotheses and all the data outcomes at once. This means the posterior distribution described by $P(H|D)$ is not a single probability distribution but many. For a *single* hypothesis, we would have something like:\n",
    "\n",
    "$P(h1|D) = \\frac{P(D|h1)P(h1)}{P(D)}$\n",
    "\n",
    "Note that the denominator does entail all hypotheses. We often end up having to use the rule of Total Probability here:\n",
    "\n",
    "$P(h1|D) = \\frac{P(D|h1)P(h1)}{P(D|h1)P(h1) + P(D|h2)P(h2) + \\dots + P(D|h_n)P(h_n)}$\n",
    "\n",
    "Bayes Rule is particularly important to data science because it says exactly how we should change our degree of certainty given new evidence. It also plays a foundational role in several machine learning techniques (Naive Bayes Classifier, Bayesian Belief Networks). It is also the main formula for Bayesian *inference*.\n",
    "\n",
    "We will spend an entire later chapter on Bayes Rule because of its central role in inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "120px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
